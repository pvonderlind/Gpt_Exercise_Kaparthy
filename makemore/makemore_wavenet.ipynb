{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Part 6: WaveNet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.1 Base Code from prev. exercise (improved a bit)\n",
    "As base, we will use the building blocks we created in the gradients exercise before:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r', encoding='utf-8').read().splitlines()\n",
    "words[:8]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos.keys())\n",
    "print(itos)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Note__: Increase block_size to 8 to increase the context size!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "block_size = 8 # Offset of context\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        #print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            #print(''.join(itos[i] for i in context), '-->', itos[ix])\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class Linear:\n",
    "\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.weights = torch.randn((fan_in, fan_out)) / fan_in ** 0.5\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weights\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weights] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "class BatchNorm1d:\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        # params\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        # buffers\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            # Note that this diverges from torch standard, we have N,L,C and they use N,C,L!\n",
    "            if x.ndim == 2:\n",
    "                dim = 0\n",
    "            elif x.ndim == 3:\n",
    "                dim = (0, 1)\n",
    "            xmean = x.mean(dim, keepdim=True)\n",
    "            xvar = x.var(dim, keepdim=True, unbiased=True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "\n",
    "        # Update buffers\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = self.running_mean * (1 - self.momentum) + self.momentum * xmean\n",
    "                self.running_var = self.running_var * (1 - self.momentum) + self.momentum * xvar\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Embedding:\n",
    "\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "\n",
    "    def __call__(self, IX):\n",
    "        self.out = self.weight[IX]\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "\n",
    "class FlattenConsecutive:\n",
    "\n",
    "    def __init__(self, n):\n",
    "        \"\"\"\n",
    "        :param n: Number of consecutive character embeddings.\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        x = x.view(B, T // self.n, C*self.n)\n",
    "        # Remove the 1 dim and proceed as before the re-write if we have a solo dim 1.\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Sequential:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Note__: We discard the generator here by setting a manual seed for torch overall, which does the same thing\n",
    "globally!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "torch.manual_seed(42);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialise a very basic network as sanity check.\n",
    "\n",
    "__Notes__:\n",
    "* __Torch Containers__: We can use a torch container (ae. Sequential, ...) instead of a list for our layers. This just\n",
    "passes the input to all the contained classes by using their __call__ property!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76579\n"
     ]
    }
   ],
   "source": [
    "n_embd = 24\n",
    "n_hidden = 128\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, n_embd),\n",
    "    FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    FlattenConsecutive(2), Linear(n_hidden * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    FlattenConsecutive(2), Linear(n_hidden * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, vocab_size)\n",
    "])\n",
    "\n",
    "# For intuition of the gain for initialisation look at the diagnostic plots below!!\n",
    "with torch.no_grad():\n",
    "    # make last layer less confident\n",
    "    model.layers[-1].weights *= 0.1\n",
    "\n",
    "\n",
    "parameters = model.parameters()\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3216\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # Create a minibatch from X\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # FORWARD PASS\n",
    "    logits = model(Xb)\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "    # BACKWARD PASS\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    lr = 0.1 if i < 100000 else 0.01 # LR decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "\n",
    "    lossi.append(loss.log10().item())\n",
    "    break # Comment out after debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Note__: __Loss plot very noisy, we will fix this now!!__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 6.1.1 Fixing the noisy loss plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that if we average the loss representation over each 1000 batches\n",
    "a nicer representation emerges. We can see the effect of the learning rate\n",
    "decay at 100.000 steps, which drastically decreases the loss:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 1000]' is invalid for input of size 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlossi\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mmean(\u001B[38;5;241m1\u001B[39m))\n",
      "\u001B[1;31mRuntimeError\u001B[0m: shape '[-1, 1000]' is invalid for input of size 1"
     ]
    }
   ],
   "source": [
    "plt.plot(torch.tensor(lossi).view(-1, 1000).mean(1));"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.training = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@torch.no_grad() # Decorate to remove gradient tracking\n",
    "def split_loss(split):\n",
    "    x, y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte)\n",
    "    }[split]\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        logits = model(torch.tensor([context]))\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, generator=g, num_samples=1).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(itos[i] for i in out))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.2 WaveNet: Scaling the MLP up\n",
    "\n",
    "Since we see that the validation and training loss still do not diverge that much,\n",
    "we can be sure that we do not overfit currently. Thus, we can increase the model\n",
    "capacity by increasing its size (parameter count)!\n",
    "\n",
    "__Since we crush our embeddings so fast, it would be nonsensical to increase the layer size\n",
    "or increase the model depth.__\n",
    "\n",
    "Therefore, we introduce the idea behind the WaveNet architecture, which __gradually decreases\n",
    "the embedding size without the crush in the initial first layer!__\n",
    "(This is similar to how a convolutional network functions)\n",
    "\n",
    "\n",
    "__Notes__:\n",
    "* __PyTorch matmal__: Is __very powerful!__ It can matmul tensors up to abitrary dimensions, as it\n",
    "__only treats the last dimension as data dimensions__, with all before as batch dimension. This way,\n",
    "we can ae. matmul a (4,5,2,3,80) tensor with a (80,200) tensor and get a (4,5,2,3,200) tensor as output!\n",
    "* __Why Conv is more performant than a loop with linear kernels__: Convolutional layers get processed in\n",
    "parallel directly by CUDA with a CUDA kernel. This is way faster than our current implementation!\n",
    "\n",
    "__Andrej's tips for practice__:\n",
    "* __Dev process__: Develop the code first in a juptier notebook to get shapes etc. right and be able\n",
    "to interactively debug the training/model building process. Then copy the code over to actual python\n",
    "scripts and make your experiment with the code there!\n",
    "* __Experiments__: Do lots of optimizations with lots of plots for train/val, gradients, grid/random-search ..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Intuition__:The idea is now using the pytorch matmul function in parallel to process bi-gram groups of our\n",
    "characters contexts."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}